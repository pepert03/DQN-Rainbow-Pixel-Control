{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Introducción a Deep Q-Learning (DQN)\n",
    "\n",
    "Bienvenido a este tutorial sobre **Deep Q-Network (DQN)**. Si eres nuevo en el Aprendizaje por Refuerzo (Reinforcement Learning - RL), ¡estás en el lugar correcto! Vamos a desglosar los conceptos paso a paso, desde lo básico hasta la implementación de un agente capaz de aprender a jugar un videojuego simple.\n",
    "\n",
    "## 1. ¿Qué es el Aprendizaje por Refuerzo (RL)?\n",
    "\n",
    "Imagina que estás enseñando a un perro a sentarse. \n",
    "- Si se sienta cuando se lo ordenas, le das una galleta (recompensa positiva).\n",
    "- Si no lo hace, no recibe nada (o quizás un leve regaño, recompensa negativa).\n",
    "Con el tiempo, el perro aprende que la acción \"sentarse\" ante la orden conduce a una recompensa.\n",
    "\n",
    "En RL, tenemos:\n",
    "- **Agente**: El que aprende (el perro, o nuestro algoritmo).\n",
    "- **Entorno (Environment)**: El mundo con el que interactúa (la habitación, nosotros).\n",
    "- **Estado (State, $s$)**: La situación actual (el perro está de pie, nosotros decimos \"sit\").\n",
    "- **Acción (Action, $a$)**: Lo que hace el agente (sentarse, ladrar, correr).\n",
    "- **Recompensa (Reward, $r$)**: La retroalimentación (la galleta).\n",
    "\n",
    "El objetivo del agente es maximizar la suma total de recompensas a lo largo del tiempo. Esto se llama el **recompensa acumulada** o **retorno** ($G_t$).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. De Q-Learning a Deep Q-Learning\n",
    "\n",
    "### Q-Learning (El enfoque clásico)\n",
    "Antes de \"Deep\" RL, existía **Q-Learning**. La idea central es aprender una función $Q(s, a)$ que nos diga: \n",
    "*\"Si estoy en el estado $s$ y tomo la acción $a$, ¿cuánta recompensa total espero obtener al final?\"*\n",
    "\n",
    "Imagina una tabla (tabla Q) donde las filas son estados y las columnas son acciones. Llenamos esta tabla con valores estimados. Cuando el agente tiene que actuar, simplemente mira la fila de su estado actual y elige la acción con el valor más alto.\n",
    "\n",
    "**La Ecuación de Bellman** para actualizar esta tabla es:\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "Donde:\n",
    "- $\\alpha$: Tasa de aprendizaje (cuánto \"confiamos\" en la nueva información).\n",
    "- $\\gamma$: Factor de descuento (cuánto nos importa el futuro vs. el presente).\n",
    "- $s'$: El siguiente estado al que llegamos.\n",
    "- $\\max_{a'} Q(s', a')$: La mejor estimación de futuro desde el nuevo estado.\n",
    "\n",
    "### El Problema\n",
    "Si el entorno es simple (como un laberinto pequeño), una tabla funciona. Pero, env videojuego como Atari (donde el estado son los pixeles de la pantalla), hay infinitos estados posibles. ¡No podemos tener una tabla con trillones de filas!\n",
    "\n",
    "### La Solución: Deep Q-Network (DQN)\n",
    "En lugar de una tabla, usamos una **Red Neuronal** para aproximar la función Q.\n",
    "$$Q(s, a; \\theta) \\approx Q^*(s, a)$$\n",
    "La red toma el estado $s$ como entrada y devuelve los valores Q para todas las acciones posibles.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Los Trucos de DQN\n",
    "\n",
    "Entrenar una red neuronal para RL es inestable. DQN introdujo dos trucos clave para hacerlo funcionar:\n",
    "\n",
    "### 1. Experience Replay (Repetición de Experiencia)\n",
    "En lugar de aprender de la experiencia inmediatamente y descartarla (como en Q-Learning estándar), guardamos las transiciones $(s, a, r, s', done)$ en una memoria (buffer). Durante el entrenamiento, tomamos un **lote aleatorio** de experiencias pasadas para entrenar la red.\n",
    "- **Por qué ayuda:** Rompe la correlación entre muestras consecutivas (los datos consecutivos son muy similares, lo que confunde a la red).\n",
    "\n",
    "### 2. Target Network (Red Objetivo)\n",
    "En la ecuación de Bellman, el objetivo ($r + \\gamma \\max Q(s', a')$) cambia constantemente porque usamos la misma red para calcular el valor objetivo y el valor predicho. Es como tratar de golpear un blanco en movimiento.\n",
    "- **Solución:** Creamos una copia de la red llamada **Target Network**. Usamos esta red copia para calcular el objetivo y la mantenemos congelada por varios pasos, actualizándola solo ocasionalmente con los pesos de la red principal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementación Práctica\n",
    "\n",
    "Vamos a implementar DQN para resolver el entorno **CartPole-v1** de Gymnasium. El objetivo es equilibrar un palo sobre un carro moviéndolo de izquierda a derecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configuración para usar GPU si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La Red Neuronal (Q-Network)\n",
    "Una red simple que toma el estado (4 valores en CartPole: posición, velocidad, ángulo, velocidad angular) y devuelve un valor para cada acción (2 acciones: izquierda, derecha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # Red simple: Entrada -> 128 neuronas -> 128 neuronas -> Salida\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer\n",
    "Una estructura para guardar y muestrear experiencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Guardar una transición\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Obtener un lote aleatorio\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros y Configuración\n",
    "Definimos los parámetros clave del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "BATCH_SIZE = 128        # Tamaño del lote para entrenar\n",
    "GAMMA = 0.99            # Factor de descuento (valoramos el futuro)\n",
    "EPS_START = 0.9         # Epsilon inicial (probabilidad de explorar)\n",
    "EPS_END = 0.05          # Epsilon final\n",
    "EPS_DECAY = 1000        # Qué tan rápido decae Epsilon\n",
    "TAU = 0.005             # Tasa de actualización suave de la red objetivo\n",
    "LR = 1e-4               # Tasa de aprendizaje (Learning Rate)\n",
    "\n",
    "# Inicializar entorno\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "n_actions = env.action_space.n\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "# Redes (Policy y Target)\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict()) # Copiamos pesos iniciales\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para elegir acción (Epsilon-Greedy)\n",
    "Con probabilidad $\\epsilon$ elegimos una acción aleatoria (exploración), de lo contrario elegimos la mejor acción según nuestra red (explotación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    # Decaimiento de epsilon\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) devolverá el valor más grande. La segunda columna del resultado es el índice\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Entrenamiento (Un paso de optimización)\n",
    "Aquí ocurre la magia matemática:\n",
    "1. Muestreamos un lote de experiencias.\n",
    "2. Calculamos $Q(s, a)$ actual.\n",
    "3. Calculamos $V_{target} = r + \\gamma \\max Q(s', a')$ usando la red objetivo.\n",
    "4. Calculamos la pérdida (Loss) entre ambos.\n",
    "5. Retropropagación (Backpropagation) para actualizar la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "        \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Máscara de estados no finales (donde el episodio no terminó)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Calcular Q(s, a) - La red nos da Q para todas las acciones, seleccionamos las que tomamos\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Calcular V(s') para los siguientes estados usando la red objetivo\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    \n",
    "    # Calcular el valor Q esperado (Target)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Calcular pérdida (Huber loss es robusta a outliers)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimizar el modelo\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Clip gradients para estabilidad\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucle de Entrenamiento\n",
    "Entrenamos durante un número de episodios. Observa cómo la duración de los episodios (cuánto tiempo aguanta el palo sin caerse) debería aumentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50   # Menos episodios si estamos en CPU para que el tutorial sea rápido\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Inicializar entorno y estado\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    for t in range(1000): # Máximo 1000 pasos por episodio\n",
    "        # Seleccionar y ejecutar acción\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        \n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Guardar transición en memoria\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "\n",
    "        # Mover al siguiente estado\n",
    "        state = next_state\n",
    "\n",
    "        # Optimizar (entrenar) la red\n",
    "        optimize_model()\n",
    "\n",
    "        # Actualización suave de la red objetivo\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n",
    "\n",
    "    if i_episode % 10 == 0:\n",
    "        print(f\"Episodio {i_episode}, Duración: {episode_durations[-1]}\")\n",
    "\n",
    "print(\"¡Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Duración de Episodios (Recompensa)\")\n",
    "plt.plot(episode_durations)\n",
    "plt.xlabel(\"Episodio\")\n",
    "plt.ylabel(\"Pasos en equilibrio\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualización del Agente\n",
    "\n",
    "Vamos a ver cómo se comporta nuestro agente entrenado. Generaremos una animación de un episodio.\n",
    "Nota: Esto requiere `matplotlib` para la animación y `ipython` para mostrarla en el notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def display_video(frames):\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    plt.close()\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "# Correr un episodio y guardar frames\n",
    "env_viz = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "state, info = env_viz.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "frames = []\n",
    "for t in range(500):\n",
    "    frames.append(env_viz.render())\n",
    "    \n",
    "    # Usar la red para elegir la mejor acción (sin exploración aleatoria)\n",
    "    with torch.no_grad():\n",
    "        action = policy_net(state).max(1)[1].view(1, 1)\n",
    "        \n",
    "    observation, reward, terminated, truncated, _ = env_viz.step(action.item())\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    if terminated:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env_viz.close()\n",
    "print(f\"Visualizando episodio de {len(frames)} pasos...\")\n",
    "display_video(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusión\n",
    "\n",
    "Has implementado un agente DQN desde cero. \n",
    "- La **Red Neuronal** aproxima los valores Q.\n",
    "- El **Replay Buffer** nos permite reutilizar experiencias pasadas y estabilizar el entrenamiento.\n",
    "- La **Target Network** nos da un objetivo estable para aprender.\n",
    "\n",
    "¡Felicidades! Ahora entiendes los bloques fundamentales del Deep Reinforcement Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
